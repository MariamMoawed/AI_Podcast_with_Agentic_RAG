{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMsIRy1wNNl3Ell8YGyOEHO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MariamMoawed/AI_Podcast_with_Agentic_RAG/blob/main/rag_document_ai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "import requests\n",
        "import os\n",
        "from flask import Flask, request, jsonify\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import pipeline\n",
        "\n",
        "# Initialize Flask app\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Load embedding model for text representation\n",
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Example dialogues dataset\n",
        "dialogues = [\n",
        "    \"Hello there! How are you today?\",\n",
        "    \"I'm doing well, thank you! What about you?\",\n",
        "    \"I'm great! Have you heard about the new AI model?\",\n",
        "    \"Yes, itâ€™s fascinating how AI is evolving!\"\n",
        "]\n",
        "\n",
        "# Convert dialogues to embeddings\n",
        "embeddings = np.array([embedding_model.encode(text) for text in dialogues])\n",
        "\n",
        "dimension = embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "index.add(embeddings)\n",
        "\n",
        "# Function to retrieve relevant dialogues\n",
        "def retrieve_relevant_dialogue(query, top_k=2):\n",
        "    query_embedding = embedding_model.encode(query).reshape(1, -1)\n",
        "    distances, indices = index.search(query_embedding, top_k)\n",
        "    return [dialogues[i] for i in indices[0]]\n",
        "\n",
        "# Load Llama 3 (Placeholder: Use appropriate API or local model)\n",
        "llama_pipeline = pipeline(\"text-generation\", model=\"meta-llama/Llama-3\")\n",
        "\n",
        "def generate_dialogue(context):\n",
        "    prompt = f\"Character 1: {context[0]}\\nCharacter 2: \"\n",
        "    response = llama_pipeline(prompt, max_length=50, num_return_sequences=1)\n",
        "    return response[0]['generated_text']\n",
        "\n",
        "# ElevenLabs API integration for voice synthesis\n",
        "ELEVENLABS_API_KEY = \"your_api_key_here\"\n",
        "ELEVENLABS_VOICE_ID = \"your_voice_id_here\"\n",
        "\n",
        "def text_to_speech(text, filename=\"output.mp3\"):\n",
        "    url = \"https://api.elevenlabs.io/v1/text-to-speech\"\n",
        "    headers = {\"xi-api-key\": ELEVENLABS_API_KEY, \"Content-Type\": \"application/json\"}\n",
        "    payload = {\"text\": text, \"voice_id\": ELEVENLABS_VOICE_ID}\n",
        "    response = requests.post(url, json=payload, headers=headers)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        with open(filename, \"wb\") as audio_file:\n",
        "            audio_file.write(response.content)\n",
        "        print(f\"Audio saved as {filename}\")\n",
        "    else:\n",
        "        print(\"Error in voice synthesis\", response.text)\n",
        "\n",
        "# API Endpoint to process user query\n",
        "@app.route(\"/generate\", methods=[\"POST\"])\n",
        "def generate():\n",
        "    data = request.json\n",
        "    query = data.get(\"query\", \"\")\n",
        "\n",
        "    retrieved_context = retrieve_relevant_dialogue(query)\n",
        "    generated_response = generate_dialogue(retrieved_context)\n",
        "    text_to_speech(generated_response, \"output.mp3\")\n",
        "\n",
        "    return jsonify({\n",
        "        \"retrieved_context\": retrieved_context,\n",
        "        \"generated_response\": generated_response,\n",
        "        \"audio_file\": \"output.mp3\"\n",
        "    })\n",
        "\n",
        "# Run the application\n",
        "if __name__ == \"__main__\":\n",
        "    app.run(host=\"0.0.0.0\", port=5000, debug=True)\n"
      ],
      "metadata": {
        "id": "vx5ieVk6scIh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}